{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image, sequence\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images_dir = os.listdir(\"F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/\")\n",
    "\n",
    "images_path = 'F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/Images/'\n",
    "captions_path = 'F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "train_path = 'F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
    "val_path = 'F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'\n",
    "test_path = 'F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt'\n",
    "\n",
    "captions = open(captions_path, 'r').read().split(\"\\n\")\n",
    "x_train = open(train_path, 'r').read().split(\"\\n\")\n",
    "x_val = open(val_path, 'r').read().split(\"\\n\")\n",
    "x_test = open(test_path, 'r').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading captions as values and images as key in dictionary\n",
    "tokens = {}\n",
    "\n",
    "for ix in range(len(captions)-1):\n",
    "    temp = captions[ix].split(\"#\")\n",
    "    if temp[0] in tokens:\n",
    "        tokens[temp[0]].append(temp[1][2:])\n",
    "    else:\n",
    "        tokens[temp[0]] = [temp[1][2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying an image and captions given to it\n",
    "temp = captions[10].split(\"#\")\n",
    "from IPython.display import Image, display\n",
    "z = Image(filename=images_path+temp[0])\n",
    "display(z)\n",
    "\n",
    "for ix in range(len(tokens[temp[0]])):\n",
    "    print(tokens[temp[0]][ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, test and validation dataset files with header as 'image_id' and 'captions'\n",
    "train_dataset = open('flickr_8k_train_dataset.txt','wb')\n",
    "train_dataset.write(b\"image_id\\tcaptions\\n\")\n",
    "\n",
    "val_dataset = open('flickr_8k_val_dataset.txt','wb')\n",
    "val_dataset.write(b\"image_id\\tcaptions\\n\")\n",
    "\n",
    "test_dataset = open('flickr_8k_test_dataset.txt','wb')\n",
    "test_dataset.write(b\"image_id\\tcaptions\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the above created files for train, test and validation dataset with image ids and captions for each of these images\n",
    "for img in x_train:\n",
    "    if img == '':\n",
    "        continue\n",
    "    for capt in tokens[img]:\n",
    "        caption = \"<start> \"+ capt + \" <end>\"\n",
    "        train_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n",
    "        train_dataset.flush()\n",
    "train_dataset.close()\n",
    "\n",
    "for img in x_test:\n",
    "    if img == '':\n",
    "        continue\n",
    "    for capt in tokens[img]:\n",
    "        caption = \"<start> \"+ capt + \" <end>\"\n",
    "        test_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n",
    "        test_dataset.flush()\n",
    "test_dataset.close()\n",
    "\n",
    "for img in x_val:\n",
    "    if img == '':\n",
    "        continue\n",
    "    for capt in tokens[img]:\n",
    "        caption = \"<start> \"+ capt + \" <end>\"\n",
    "        val_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n",
    "        val_dataset.flush()\n",
    "val_dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 50 layer Residual Network Model and getting the summary of the model\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"<a href=\"http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\">ResNet50 Architecture</a>\"\"\"))\n",
    "model = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
    "model.summary()\n",
    "# Note: For more details on ResNet50 architecture you can click on hyperlink given below\n",
    "\n",
    "# Helper function to process images\n",
    "def preprocessing(img_path):\n",
    "    im = image.load_img(img_path, target_size=(224,224,3))\n",
    "    im = image.img_to_array(im)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    return im\n",
    "\n",
    "train_data = {}\n",
    "ctr=0\n",
    "for ix in x_train:\n",
    "    if ix == \"\":\n",
    "        continue\n",
    "    if ctr >= 3000:\n",
    "        break\n",
    "    ctr+=1\n",
    "    if ctr%1000==0:\n",
    "        print(ctr)\n",
    "    path = images_path + ix\n",
    "    img = preprocessing(path)\n",
    "    pred = model.predict(img).reshape(2048)\n",
    "    train_data[ix] = pred\n",
    "    \n",
    "train_data['2513260012_03d33305cf.jpg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening train_encoded_images.p file and dumping it's content\n",
    "with open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n",
    "    pickle.dump(train_data, pickle_f )\n",
    "    \n",
    "# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'\n",
    "pd_dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\n",
    "ds = pd_dataset.values\n",
    "print(ds.shape)\n",
    "\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the captions from ds into a list\n",
    "sentences = []\n",
    "for ix in range(ds.shape[0]):\n",
    "    sentences.append(ds[ix, 1])\n",
    "    \n",
    "print(len(sentences))\n",
    "\n",
    "# First 5 captions stored in sentences\n",
    "sentences[:16]\n",
    "\n",
    "# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list\n",
    "words = [i.split() for i in sentences]\n",
    "\n",
    "# Creating a list of all unique words\n",
    "unique = []\n",
    "for i in words:\n",
    "    unique.extend(i)\n",
    "unique = list(set(unique))\n",
    "\n",
    "print(len(unique))\n",
    "\n",
    "vocab_size = len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "word_2_indices = {val:index for index, val in enumerate(unique)}\n",
    "indices_2_word = {index:val for index, val in enumerate(unique)}\n",
    "\n",
    "word_2_indices['UNK'] = 0\n",
    "word_2_indices['raining'] = 8253\n",
    "\n",
    "indices_2_word[0] = 'UNK'\n",
    "indices_2_word[8253] = 'raining'\n",
    "\n",
    "print(word_2_indices['<start>'])\n",
    "print(indices_2_word[4011])\n",
    "print(word_2_indices['<end>'])\n",
    "print(indices_2_word[8051])\n",
    "\n",
    "vocab_size = len(word_2_indices.keys())\n",
    "print(vocab_size)\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for i in sentences:\n",
    "    i = i.split()\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "print(max_len)\n",
    "\n",
    "padded_sequences, subsequent_words = [], []\n",
    "\n",
    "for ix in range(ds.shape[0]):\n",
    "    partial_seqs = []\n",
    "    next_words = []\n",
    "    text = ds[ix, 1].split()\n",
    "    text = [word_2_indices[i] for i in text]\n",
    "    for i in range(1, len(text)):\n",
    "        partial_seqs.append(text[:i])\n",
    "        next_words.append(text[i])\n",
    "    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n",
    "\n",
    "    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n",
    "    \n",
    "    #Vectorization\n",
    "    for i,next_word in enumerate(next_words):\n",
    "        next_words_1hot[i, next_word] = 1\n",
    "        \n",
    "    padded_sequences.append(padded_partial_seqs)\n",
    "    subsequent_words.append(next_words_1hot)\n",
    "    \n",
    "padded_sequences = np.asarray(padded_sequences)\n",
    "subsequent_words = np.asarray(subsequent_words)\n",
    "\n",
    "print(padded_sequences.shape)\n",
    "print(subsequent_words.shape)\n",
    "\n",
    "print(padded_sequences[0])\n",
    "\n",
    "for ix in range(len(padded_sequences[0])):\n",
    "    for iy in range(max_len):\n",
    "        print(indices_2_word[padded_sequences[0][ix][iy]],)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(len(padded_sequences[0]))\n",
    "\n",
    "num_of_images = 2000\n",
    "\n",
    "captions = np.zeros([0, max_len])\n",
    "next_words = np.zeros([0, vocab_size])\n",
    "\n",
    "for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):\n",
    "    captions = np.concatenate([captions, padded_sequences[ix]])\n",
    "    next_words = np.concatenate([next_words, subsequent_words[ix]])\n",
    "\n",
    "np.save(\"captions.npy\", captions)\n",
    "np.save(\"next_words.npy\", next_words)\n",
    "\n",
    "print(captions.shape)\n",
    "print(next_words.shape)\n",
    "\n",
    "with open('F:/LBSIM/Trimester-5/Deep Learning/Project/Dataset_5Dec2019/Flickr_Data/train_encoded_images.p', 'rb') as f:\n",
    "    encoded_images = pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for ix in range(ds.shape[0]):\n",
    "    if ds[ix, 0].encode() in encoded_images.keys():\n",
    "#         print(ix, encoded_images[ds[ix, 0].encode()])\n",
    "        imgs.append(list(encoded_images[ds[ix, 0].encode()]))\n",
    "\n",
    "imgs = np.asarray(imgs)\n",
    "print(imgs.shape)\n",
    "\n",
    "images = []\n",
    "\n",
    "for ix in range(num_of_images):\n",
    "    for iy in range(padded_sequences[ix].shape[0]):\n",
    "        images.append(imgs[ix])\n",
    "        \n",
    "images = np.asarray(images)\n",
    "\n",
    "np.save(\"images.npy\", images)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "image_names = []\n",
    "\n",
    "for ix in range(num_of_images):\n",
    "    for iy in range(padded_sequences[ix].shape[0]):\n",
    "        image_names.append(ds[ix, 0])\n",
    "        \n",
    "image_names = np.asarray(image_names)\n",
    "\n",
    "np.save(\"image_names.npy\", image_names)\n",
    "\n",
    "print(len(image_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "\n",
    "captions = np.load(\"captions.npy\")\n",
    "next_words = np.load(\"next_words.npy\")\n",
    "\n",
    "print(captions.shape)\n",
    "print(next_words.shape)\n",
    "\n",
    "images = np.load(\"images.npy\")\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "imag = np.load(\"image_names.npy\")\n",
    "        \n",
    "print(imag.shape)\n",
    "\n",
    "embedding_size = 128\n",
    "max_len = 40\n",
    "\n",
    "image_model = Sequential()\n",
    "\n",
    "image_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\n",
    "image_model.add(RepeatVector(max_len))\n",
    "\n",
    "image_model.summary()\n",
    "\n",
    "language_model = Sequential()\n",
    "\n",
    "language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\n",
    "language_model.add(LSTM(256, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(embedding_size)))\n",
    "\n",
    "language_model.summary()\n",
    "\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "x = LSTM(128, return_sequences=True)(conca)\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs = out)\n",
    "\n",
    "# model.load_weights(\"../input/model_weights.h5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit([images, captions], next_words, batch_size=512, epochs=107)\n",
    "\n",
    "model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "def preprocessing(img_path):\n",
    "    im = image.load_img(img_path, target_size=(224,224,3))\n",
    "    im = image.img_to_array(im)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    return im\n",
    "\n",
    "def get_encoding(model, img):\n",
    "    image = preprocessing(img)\n",
    "    pred = model.predict(image).reshape(2048)\n",
    "    return pred\n",
    "\n",
    "resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\n",
    "\n",
    "img = \"F:/LBSIM/Trimester-5/Deep Learning/Project/Final Eval/Pics/00010.jpg\"\n",
    "\n",
    "test_img = get_encoding(resnet, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_captions(image):\n",
    "    start_word = [\"<start>\"]\n",
    "    while True:\n",
    "        par_caps = [word_2_indices[i] for i in start_word]\n",
    "        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n",
    "        preds = model.predict([np.array([image]), np.array(par_caps)])\n",
    "        word_pred = indices_2_word[np.argmax(preds[0])]\n",
    "        start_word.append(word_pred)\n",
    "        \n",
    "        if word_pred == \"<end>\" or len(start_word) > max_len:\n",
    "            break\n",
    "            \n",
    "    return ' '.join(start_word[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Argmax_Search = predict_captions(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Image(filename=img)\n",
    "display(z)\n",
    "\n",
    "print(Argmax_Search)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
